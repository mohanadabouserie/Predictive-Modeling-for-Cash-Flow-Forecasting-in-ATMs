{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import BayesianRidge, RidgeCV, Lasso\n",
    "from sklearn.linear_model import RANSACRegressor, Ridge, LinearRegression\n",
    "from lazypredict.Supervised import LazyRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV data into a Pandas DataFrame\n",
    "data = pd.read_csv('AggregatedData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Transaction Date' to datetime\n",
    "data['Transaction Date'] = data['Transaction Date'].str.replace('/', '-', regex=True)\n",
    "data['Transaction Date'] = pd.to_datetime(data['Transaction Date'], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NULL values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display rows with NULL values (if any)\n",
    "data[data['Transaction Date'].isna() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract day of the week to make it a numeric value\n",
    "data['DayOfWeek'] = data['Transaction Date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by 'ATM Name' and 'Transaction Date', and sum the cash flow values\n",
    "atm_data = data.groupby(['ATM Name', 'Transaction Date']).sum().reset_index()\n",
    "\n",
    "# Use LabelEncoder for 'Festival Religion' and 'Holiday Sequence' columns\n",
    "label_encoder = LabelEncoder()\n",
    "atm_data['Festival Religion'] = label_encoder.fit_transform(atm_data['Festival Religion'])\n",
    "atm_data['Holiday Sequence'] = label_encoder.fit_transform(atm_data['Holiday Sequence'])\n",
    "\n",
    "# Preprocess 'Weekday' column to ensure consistent capitalization\n",
    "atm_data['Weekday'] = atm_data['Weekday'].str.lower().str.capitalize()\n",
    "\n",
    "# Use get_dummies for 'Weekday' and 'Working Day' columns\n",
    "dummy_columns = pd.get_dummies(atm_data[['Weekday', 'Working Day']], columns=['Weekday', 'Working Day'])\n",
    "\n",
    "# Convert boolean columns to integer values (0 and 1)\n",
    "dummy_columns = dummy_columns.astype(int)\n",
    "\n",
    "# Concatenate the dummy columns with the original data\n",
    "atm_data = pd.concat([atm_data, dummy_columns], axis=1)\n",
    "\n",
    "atm_data['Working Day'] = label_encoder.fit_transform(atm_data['Working Day'])\n",
    "\n",
    "# Display the data\n",
    "atm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the columns that I want to normalize\n",
    "columns_after_normalize = ['Norm Total amount Withdrawn', 'Norm Amount withdrawn XYZ Card', 'Norm Amount withdrawn Other Card']\n",
    "columns_to_normalize = ['Total amount Withdrawn', 'Amount withdrawn XYZ Card', 'Amount withdrawn Other Card']\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize the columns and create new columns with normalized values\n",
    "atm_data[columns_after_normalize] = scaler.fit_transform(atm_data[columns_to_normalize])\n",
    "atm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the data to a new DataFrame and set the index to 'Transaction Date'\n",
    "atm_data_day = atm_data.copy()\n",
    "atm_data_day = atm_data.set_index(['Transaction Date'])\n",
    "atm_data_day.sort_index(inplace=True)\n",
    "atm_data_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the No Of XYZ Card Withdrawals and No Of Other Card Withdrawals over time\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(atm_data_day['No Of XYZ Card Withdrawals'], label='No Of XYZ Card Withdrawals')\n",
    "plt.plot(atm_data_day['No Of Other Card Withdrawals'], label='No Of Other Card Withdrawals')\n",
    "plt.xlabel('Transaction Date')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Number of XYZ Card Withdrawals and Number of Other Card Withdrawals Over Time')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "        The line plot provides a visualization of the trends in the number of withdrawals for both XYZ Card \n",
    "        and Other Card transactions over time. One notable observation is the big decline in the number of \n",
    "        XYZ Card withdrawals and the increase of the Other Card withdrawals compared to XYZ Card withdrawals starting in 2015. \n",
    "        This change in transaction volume for both card types suggests a potential shift in customer behavior\n",
    "        or external factors affecting ATM usage.\n",
    "        \n",
    "        From a predictive perspective, those fluctuations in transaction volume can impact the total amount\n",
    "        withdrawn for a specific ATM, so those features can be crucial for accurately predicting the future\n",
    "        cash flow of ATMs.\n",
    "        \n",
    "        Incorporating these insights into predictive models can help improve the accuracy of predictions and\n",
    "        enable proactive measures to address any challenges posed by changing transaction patterns. Analyzing\n",
    "        historical trends and identifying potential drivers behind them is essential for building robust \n",
    "        predictive models that capture the complexities of ATM cash flow dynamics.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Line Plot for XYZ Card and Other Card Withdrawn Amounts Over Time\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(atm_data_day['Amount withdrawn XYZ Card'], label='Amount withdrawn XYZ Card')\n",
    "plt.plot(atm_data_day['Amount withdrawn Other Card'], label='Amount withdrawn Other Card')\n",
    "plt.xlabel('Transaction Date')\n",
    "plt.ylabel('Amount')\n",
    "plt.title('Amount Withdrawn for XYZ Card and Other Card Transactions Over Time')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "        The line plot provides a clear visualization of the trends in the withdrawn amount for \n",
    "        both XYZ Card and Other Card transactions over time. Throughout the depicted time frame,\n",
    "        the withdrawn amount for XYZ Card transactions appears to have a more significant \n",
    "        presence compared to Other Card transactions. This trend holds except for the start of \n",
    "        2017, where the withdrawn amount for Other Card transactions begins to rise, eventually \n",
    "        surpassing the amount for XYZ Card transactions.\n",
    "\n",
    "        This divergence around the beginning of 2017 further underscores the dynamic nature of \n",
    "        customer behavior and external factors affecting ATM usage. While the trend may seem \n",
    "        aligned for the majority of the period, this shift in the latter part of the timeline \n",
    "        highlights the necessity of analyzing comprehensive data, including both transaction \n",
    "        volume and withdrawn amounts, to build accurate predictive models for ATM cash flow. \n",
    "        The insights from both transaction volume and withdrawn amounts serve as crucial features \n",
    "        for predictive modeling, enabling the development of accurate projections and strategies \n",
    "        that respond to changing patterns in ATM usage.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Line Plot for Cash Flow Over Time for ATMs\n",
    "plt.figure(figsize=(20, 7))\n",
    "plt.plot(atm_data_day['Total amount Withdrawn'])\n",
    "plt.xlabel('Transaction Date')\n",
    "plt.ylabel('Total amount Withdrawn')\n",
    "plt.title('Cash Flow Over Time for ATMs')\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "        The line plot visually illustrates the cash flow trends over time for ATMs. \n",
    "        The plot showcases that the total amount withdrawn from ATMs exhibits a \n",
    "        relatively consistent pattern, with some noticeable deviations. Notably, \n",
    "        there is a distinct peak in cash flow observed from around 2013 to 2015, \n",
    "        indicating a period of increased ATM usage and withdrawals. However, \n",
    "        the preiod just before the start of 2017, there is a visible drop in the \n",
    "        cash flow, suggesting a potential shift in customer behavior or external \n",
    "        factors affecting cash withdrawal patterns. The overall stability of cash \n",
    "        flow, punctuated by these significant variations, highlights the importance \n",
    "        of analyzing historical trends and identifying anomalous periods for more \n",
    "        accurate prediction and proactive management of ATM cash flow dynamics.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the data to group by month and calculate the sum of transactions\n",
    "atm_data_monthly = atm_data.copy()\n",
    "atm_data_monthly['Transaction Date'] = pd.to_datetime(atm_data['Transaction Date']).dt.to_period('M')\n",
    "atm_data_monthly = atm_data_monthly.groupby(['ATM Name', 'Transaction Date']).sum().reset_index()\n",
    "\n",
    "# List of columns to drop\n",
    "columns_to_drop = ['Weekday', 'Festival Religion', 'Working Day', 'Holiday Sequence', 'DayOfWeek', 'Norm Total amount Withdrawn', 'Norm Amount withdrawn XYZ Card', 'Norm Amount withdrawn Other Card', 'Weekday_Friday', 'Weekday_Monday', 'Weekday_Saturday', 'Weekday_Sunday', 'Weekday_Thursday', 'Weekday_Tuesday', 'Weekday_Wednesday', 'Working Day_H', 'Working Day_W']\n",
    "\n",
    "# Drop the specified columns\n",
    "atm_data_monthly = atm_data_monthly.drop(columns=columns_to_drop)\n",
    "\n",
    "# Set 'Transaction Date' as the index\n",
    "atm_data_monthly.set_index('Transaction Date', inplace=True)\n",
    "\n",
    "# Normalize the columns and store normalized values in separate columns using MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "columns_to_normalize = ['Total amount Withdrawn', 'Amount withdrawn XYZ Card', 'Amount withdrawn Other Card']\n",
    "for column in columns_to_normalize:\n",
    "    atm_data_monthly[column + '_normalized'] = scaler.fit_transform(atm_data_monthly[[column]])\n",
    "\n",
    "# Display the resulting dataset with monthly aggregated data and normalized columns\n",
    "atm_data_monthly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cash flow over time for all ATMs using bar plot (Monthly)\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.bar(atm_data_monthly.index.to_timestamp(), atm_data_monthly['Total amount Withdrawn'], width=20)\n",
    "plt.xlabel('Transaction Month')\n",
    "plt.ylabel('Total amount Withdrawn')\n",
    "plt.title('Cash Flow Over Time for ATMs')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the data to group by year and calculate the sum of transactions\n",
    "atm_data_yearly = atm_data.copy()\n",
    "atm_data_yearly['Transaction Date'] = pd.to_datetime(atm_data['Transaction Date']).dt.to_period('Y')  # Convert to Period with only year\n",
    "atm_data_yearly = atm_data_yearly.groupby(['ATM Name', 'Transaction Date']).sum().reset_index()\n",
    "\n",
    "# List of columns to drop\n",
    "columns_to_drop = ['Weekday', 'Festival Religion', 'Working Day', 'Holiday Sequence', 'DayOfWeek', 'Norm Total amount Withdrawn', 'Norm Amount withdrawn XYZ Card', 'Norm Amount withdrawn Other Card', 'Weekday_Friday', 'Weekday_Monday', 'Weekday_Saturday', 'Weekday_Sunday', 'Weekday_Thursday', 'Weekday_Tuesday', 'Weekday_Wednesday', 'Working Day_H', 'Working Day_W']\n",
    "\n",
    "# Drop the specified columns\n",
    "atm_data_yearly = atm_data_yearly.drop(columns=columns_to_drop)\n",
    "\n",
    "# Set 'Transaction Date' as the index\n",
    "atm_data_yearly.set_index('Transaction Date', inplace=True)\n",
    "\n",
    "# Normalize the columns and store normalized values in separate columns using MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "columns_to_normalize = ['Total amount Withdrawn', 'Amount withdrawn XYZ Card', 'Amount withdrawn Other Card']\n",
    "for column in columns_to_normalize:\n",
    "    atm_data_yearly[column + '_normalized'] = scaler.fit_transform(atm_data_yearly[[column]])\n",
    "\n",
    "# Display the resulting dataset with yearly aggregated data and normalized columns\n",
    "atm_data_yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cash flow over time for all ATMs using bar plot (yearly)\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.bar(atm_data_yearly.index.to_timestamp(), atm_data_yearly['Total amount Withdrawn'], width=100)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total amount Withdrawn')\n",
    "plt.title('Yearly Cash Flow for ATMs')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the atm_data to atm_data_day to remove all the changes made on atm_data_day\n",
    "atm_data_day = atm_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the columns that I want to apply the lag on\n",
    "lag_columns = ['Total amount Withdrawn', 'Amount withdrawn XYZ Card', 'Amount withdrawn Other Card', 'No Of Withdrawals', 'No Of XYZ Card Withdrawals', 'No Of Other Card Withdrawals']\n",
    "lag_shift = 1  # the number of rows that will be shifted is 1\n",
    "\n",
    "# A loop that goes each column and apply a shift by 1 row over each ATM group\n",
    "for column in lag_columns:\n",
    "    lag_column_name = f'{column}_lag_{lag_shift}'\n",
    "    atm_data_day[lag_column_name] = atm_data_day.groupby('ATM Name')[column].shift(lag_shift)\n",
    "\n",
    "# Specify the lagged columns to drop null values for because we have a row that contain NULL values at the start of each ATM group\n",
    "lagged_columns_to_drop = [f'{column}_lag_{lag_shift}' for column in lag_columns]\n",
    "atm_data_day.dropna(subset=lagged_columns_to_drop, inplace=True)\n",
    "\n",
    "# Display the updated DataFrame with the lagged columns to ensure that it has been applied correctly\n",
    "atm_data_day.head(1382)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index and remove the old index levels\n",
    "atm_data_day.reset_index(level=[0], inplace=True)\n",
    "atm_data_day.drop(columns=['index'],axis=1, inplace=True)\n",
    "\n",
    "# Display the DataFrame with the new index\n",
    "atm_data_day.head(1382)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns for which I want to compute the SMA (Simple Moving Average)\n",
    "columns_to_smooth = ['No Of Withdrawals', 'No Of XYZ Card Withdrawals', 'No Of Other Card Withdrawals', \n",
    "                     'Total amount Withdrawn', 'Amount withdrawn XYZ Card', 'Amount withdrawn Other Card']\n",
    "\n",
    "# Calculate the Simple Moving Average (SMA) with windows of 3, 7, and 10 days\n",
    "window_size_1 = 3\n",
    "window_size_2 = 7\n",
    "window_size_3 = 10\n",
    "\n",
    "for column in columns_to_smooth:\n",
    "    # For window size = 3\n",
    "    # Calculate SMA for each ATM separately\n",
    "    atm_sma_1 = atm_data_day.groupby('ATM Name')[column].rolling(window=window_size_1, min_periods = 1).mean()\n",
    "    \n",
    "    # Reindex the calculated SMA to match the index of atm_data_day\n",
    "    atm_sma_1 = atm_sma_1.reset_index(level=0, drop=True).reindex(atm_data_day.index)\n",
    "    \n",
    "    # Assign the calculated SMA to atm_data_day\n",
    "    atm_data_day[column + '_SMA_3'] = atm_sma_1\n",
    "    \n",
    "    \n",
    "    # For window size = 7\n",
    "    # Calculate SMA for each ATM separately\n",
    "    atm_sma_2 = atm_data_day.groupby('ATM Name')[column].rolling(window=window_size_2, min_periods = 1).mean()\n",
    "    \n",
    "    # Reindex the calculated SMA to match the index of atm_data_day\n",
    "    atm_sma_2 = atm_sma_2.reset_index(level=0, drop=True).reindex(atm_data_day.index)\n",
    "    \n",
    "    # Assign the calculated SMA to atm_data_day\n",
    "    atm_data_day[column + '_SMA_7'] = atm_sma_2\n",
    "    \n",
    "    \n",
    "    # For window size = 10\n",
    "     # Calculate SMA for each ATM separately\n",
    "    atm_sma_3 = atm_data_day.groupby('ATM Name')[column].rolling(window=window_size_3, min_periods = 1).mean()\n",
    "    \n",
    "    # Reindex the calculated SMA to match the index of atm_data_day\n",
    "    atm_sma_3 = atm_sma_3.reset_index(level=0, drop=True).reindex(atm_data_day.index)\n",
    "    \n",
    "    # Assign the calculated SMA to atm_data_day\n",
    "    atm_data_day[column + '_SMA_10'] = atm_sma_3\n",
    "\n",
    "\n",
    "# Display the updated DataFrame with SMA columns to ensure that it has been applied correctly\n",
    "atm_data_day.head(1382)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns for which you want to compute the EMA (Exponential Moving Average)\n",
    "columns_to_smooth = ['No Of Withdrawals', 'No Of XYZ Card Withdrawals', 'No Of Other Card Withdrawals', \n",
    "                     'Total amount Withdrawn', 'Amount withdrawn XYZ Card', 'Amount withdrawn Other Card']\n",
    "\n",
    "# Calculate the Exponential Moving Average (EMA) with spans of 10, 30, and 90  days\n",
    "span_1 = 10\n",
    "span_2 = 30\n",
    "span_3 = 90\n",
    "for column in columns_to_smooth:\n",
    "    # For span size = 10\n",
    "    # Calculate EMA for each ATM separately\n",
    "    atm_ema_1 = atm_data_day.groupby('ATM Name')[column].ewm(span=span_1, min_periods=1).mean()\n",
    "    \n",
    "    # Reindex the calculated EMA to match the index of atm_data_day\n",
    "    atm_ema_1 = atm_ema_1.reset_index(level=0, drop=True).reindex(atm_data_day.index)\n",
    "    \n",
    "    # Assign the calculated EMA to atm_data_day\n",
    "    atm_data_day[column + '_EMA_10'] = atm_ema_1\n",
    "    \n",
    "    \n",
    "    # For span size = 30\n",
    "    # Calculate EMA for each ATM separately\n",
    "    atm_ema_2 = atm_data_day.groupby('ATM Name')[column].ewm(span=span_2, min_periods=1).mean()\n",
    "    \n",
    "    # Reindex the calculated EMA to match the index of atm_data_day\n",
    "    atm_ema_2 = atm_ema_2.reset_index(level=0, drop=True).reindex(atm_data_day.index)\n",
    "    \n",
    "    # Assign the calculated EMA to atm_data_day\n",
    "    atm_data_day[column + '_EMA_30'] = atm_ema_2\n",
    "    \n",
    "    \n",
    "    # For span size = 90\n",
    "    # Calculate EMA for each ATM separately\n",
    "    atm_ema_3 = atm_data_day.groupby('ATM Name')[column].ewm(span=span_3, min_periods=1).mean()\n",
    "    \n",
    "    # Reindex the calculated EMA to match the index of atm_data_day\n",
    "    atm_ema_3 = atm_ema_3.reset_index(level=0, drop=True).reindex(atm_data_day.index)\n",
    "    \n",
    "    # Assign the calculated EMA to atm_data_day\n",
    "    atm_data_day[column + '_EMA_90'] = atm_ema_3\n",
    "    \n",
    "\n",
    "# Display the updated DataFrame with EMA columns to ensure that it has been applied correctly\n",
    "atm_data_day.head(1382)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of the DataFrame to default integer index\n",
    "atm_data_monthly = atm_data_monthly.reset_index()\n",
    "\n",
    "# Sort the DataFrame by the 'ATM Name' column in ascending order\n",
    "atm_data_monthly = atm_data_monthly.sort_values(by='ATM Name')\n",
    "\n",
    "# Sort the DataFrame by the 'Transaction Date' column in ascending order\n",
    "atm_data_monthly = atm_data_monthly.sort_values(by='Transaction Date')\n",
    "\n",
    "# Set the index of the DataFrame to a multi-index consisting of 'Transaction Date' and 'ATM Name'\n",
    "atm_data_monthly = atm_data_monthly.set_index(['Transaction Date', 'ATM Name'])\n",
    "\n",
    "atm_data_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of the DataFrame to default integer index\n",
    "atm_data_yearly = atm_data_yearly.reset_index()\n",
    "\n",
    "# Sort the DataFrame by the 'ATM Name' column in ascending order\n",
    "atm_data_yearly = atm_data_yearly.sort_values(by='ATM Name')\n",
    "\n",
    "# Sort the DataFrame by the 'Transaction Date' column in ascending order\n",
    "atm_data_yearly = atm_data_yearly.sort_values(by='Transaction Date')\n",
    "\n",
    "# Set the index of the DataFrame to a multi-index consisting of 'Transaction Date' and 'ATM Name'\n",
    "atm_data_yearly = atm_data_yearly.set_index(['Transaction Date', 'ATM Name'])\n",
    "\n",
    "atm_data_yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Transaction Date as the index for atm_data_day\n",
    "atm_data_day.set_index('Transaction Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_data_day.head(2255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for which I want to calculate correlation\n",
    "selected_columns = ['DayOfWeek', 'Festival Religion', 'Working Day', 'Holiday Sequence', 'Weekday_Friday',\n",
    "       'Weekday_Monday', 'Weekday_Saturday', 'Weekday_Sunday',\n",
    "       'Weekday_Thursday', 'Weekday_Tuesday', 'Weekday_Wednesday',\n",
    "       'Working Day_H', 'Working Day_W', 'Total amount Withdrawn_lag_1',\n",
    "       'Amount withdrawn XYZ Card_lag_1', 'Amount withdrawn Other Card_lag_1',\n",
    "       'No Of Withdrawals_lag_1', 'No Of XYZ Card Withdrawals_lag_1',\n",
    "       'No Of Other Card Withdrawals_lag_1', 'No Of Withdrawals_SMA_3',\n",
    "       'No Of Withdrawals_SMA_7', 'No Of Withdrawals_SMA_10',\n",
    "       'No Of XYZ Card Withdrawals_SMA_3', 'No Of XYZ Card Withdrawals_SMA_7',\n",
    "       'No Of XYZ Card Withdrawals_SMA_10',\n",
    "       'No Of Other Card Withdrawals_SMA_3',\n",
    "       'No Of Other Card Withdrawals_SMA_7',\n",
    "       'No Of Other Card Withdrawals_SMA_10', 'Total amount Withdrawn_SMA_3',\n",
    "       'Total amount Withdrawn_SMA_7', 'Total amount Withdrawn_SMA_10',\n",
    "       'Amount withdrawn XYZ Card_SMA_3', 'Amount withdrawn XYZ Card_SMA_7',\n",
    "       'Amount withdrawn XYZ Card_SMA_10', 'Amount withdrawn Other Card_SMA_3',\n",
    "       'Amount withdrawn Other Card_SMA_7',\n",
    "       'Amount withdrawn Other Card_SMA_10', 'No Of Withdrawals_EMA_10',\n",
    "       'No Of Withdrawals_EMA_30', 'No Of Withdrawals_EMA_90',\n",
    "       'No Of XYZ Card Withdrawals_EMA_10',\n",
    "       'No Of XYZ Card Withdrawals_EMA_30',\n",
    "       'No Of XYZ Card Withdrawals_EMA_90',\n",
    "       'No Of Other Card Withdrawals_EMA_10',\n",
    "       'No Of Other Card Withdrawals_EMA_30',\n",
    "       'No Of Other Card Withdrawals_EMA_90', 'Total amount Withdrawn_EMA_10',\n",
    "       'Total amount Withdrawn_EMA_30', 'Total amount Withdrawn_EMA_90',\n",
    "       'Amount withdrawn XYZ Card_EMA_10', 'Amount withdrawn XYZ Card_EMA_30',\n",
    "       'Amount withdrawn XYZ Card_EMA_90',\n",
    "       'Amount withdrawn Other Card_EMA_10',\n",
    "       'Amount withdrawn Other Card_EMA_30',\n",
    "       'Amount withdrawn Other Card_EMA_90']\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = atm_data_day[selected_columns].corr()\n",
    "\n",
    "# Create a heatmap-style correlation plot using matplotlib\n",
    "plt.figure(figsize=(60, 25))\n",
    "plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='none')\n",
    "plt.title('Correlation Plot')\n",
    "plt.colorbar() \n",
    "plt.xticks(range(len(selected_columns)), selected_columns, rotation=90)\n",
    "plt.yticks(range(len(selected_columns)), selected_columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation between 'Total amount Withdrawn' and selected columns\n",
    "correlation_values = atm_data_day[selected_columns].corrwith(atm_data_day['Total amount Withdrawn']).sort_values()\n",
    "\n",
    "plt.figure(figsize=(60, 25))\n",
    "correlation_values.plot(kind='bar', color='blue')\n",
    "plt.title('Correlation between Total Amount Withdrawn and Other Columns')\n",
    "plt.ylabel('Correlation')\n",
    "\n",
    "# Increase spacing between labels\n",
    "plt.subplots_adjust(bottom=0.4)\n",
    "\n",
    "plt.xticks(rotation=80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns that include 'No Of Withdrawals' or 'Total amount Withdrawn' in their names\n",
    "total_amount_cards_columns_filtered = [col for col in selected_columns if 'No Of Withdrawals' in col or 'Total amount Withdrawn' in col]\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = atm_data_day[total_amount_cards_columns_filtered].corr()\n",
    "\n",
    "# Create a heatmap-style correlation plot using matplotlib\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='none')\n",
    "plt.title('Correlation Plot')\n",
    "plt.colorbar()  # Add a colorbar for reference\n",
    "plt.xticks(range(len(total_amount_cards_columns_filtered)), total_amount_cards_columns_filtered, rotation=90)\n",
    "plt.yticks(range(len(total_amount_cards_columns_filtered)), total_amount_cards_columns_filtered)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "    The presented correlation plot offers valuable insights into the relationships among various \n",
    "    features associated with ATM cash flow dynamics. The heatmap-style visualization provides a \n",
    "    clear view of the correlation coefficients between different columns representing metrics \n",
    "    related to the number of withdrawals and total amount withdrawn for both XYZ Card and Other \n",
    "    Card transactions.\n",
    "\n",
    "    The plot reveals that there is a strong correlation between different time-based moving averages\n",
    "    (SMA and EMA) and themselves, indicating the continuity and consistency of trends in withdrawal \n",
    "    patterns over time. Moreover, there is a noticeable positive correlation between SMAs and EMAs, \n",
    "    suggesting that the smoothed averages of withdrawal metrics align closely, supporting the notion \n",
    "    of stable and predictable behavior in cash flow.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation between 'Total amount Withdrawn' and selected columns\n",
    "correlation_values = atm_data_day[total_amount_cards_columns_filtered].corrwith(atm_data_day['Total amount Withdrawn']).sort_values()\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "correlation_values.plot(kind='bar', color='blue')\n",
    "plt.title('Correlation between Total Amount Withdrawn and Other Columns')\n",
    "plt.ylabel('Correlation')\n",
    "\n",
    "# Increase spacing between labels\n",
    "plt.subplots_adjust(bottom=0.4)\n",
    "\n",
    "plt.xticks(rotation=80)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "    The presented bar plot provides valuable insights into the correlations between \n",
    "    the target variable, which is the 'Total amount Withdrawn', and various other \n",
    "    features related to ATM transactions. The y-axis represents the correlation \n",
    "    coefficients, indicating the strength and direction of the relationship.\n",
    "\n",
    "    The plot highlights a favorable correlation between the 'Total amount Withdrawn' \n",
    "    and different time-based moving averages (SMA and EMA) derived from the ATM \n",
    "    transaction metrics. The positive correlation indicates that changes in the \n",
    "    smoothed averages of transaction metrics, such as 'No Of Withdrawals_SMA_X' \n",
    "    and 'No Of Withdrawals_EMA_X', are associated with corresponding changes in \n",
    "    the total amount withdrawn from ATMs. This alignment suggests that trends and \n",
    "    patterns captured by these features provide valuable predictive insights into \n",
    "    the cash flow dynamics of ATMs. Incorporating these features into the model's \n",
    "    training can enhance its ability to capture the underlying patterns and \n",
    "    fluctuations in ATM transactions, leading to improved predictions and informed \n",
    "    decision-making.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    The presented bar plot not only reveals the strong correlation between the 'Total \n",
    "    amount Withdrawn' and various time-based moving averages (SMA and EMA) features but \n",
    "    also highlights that the correlation for the lag columns is also noteworthy. While \n",
    "    the correlation coefficients for the lag columns may not be as high as those for the \n",
    "    smoothed averages, they still indicate a meaningful relationship between historical \n",
    "    transaction metrics and the total amount withdrawn from ATMs.\n",
    "\n",
    "    The positive correlation between the lag columns (e.g., 'Total amount Withdrawn_lag_1', \n",
    "    'No Of Withdrawals_lag_1', etc.) and the 'Total amount Withdrawn' suggests that past \n",
    "    transaction data holds predictive power for forecasting the future cash flow of ATMs. \n",
    "    This finding aligns with the concept that historical transaction behavior can provide \n",
    "    insights into the trends and patterns that drive ATM usage and, consequently, the cash flow.\n",
    "\n",
    "    These insights underscore the importance of incorporating a combination of time-related \n",
    "    features, including lag columns and moving averages, into predictive models. By doing so, \n",
    "    these models can effectively capture both short-term fluctuations and longer-term trends, \n",
    "    leading to more accurate and robust predictions of ATM cash flow dynamics.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns that include 'XYZ'in their names\n",
    "XYZ_amount_cards_columns_filtered = [col for col in selected_columns if 'XYZ' in col]\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = atm_data_day[XYZ_amount_cards_columns_filtered].corr()\n",
    "\n",
    "# Create a heatmap-style correlation plot using matplotlib\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='none')\n",
    "plt.title('Correlation Plot')\n",
    "plt.colorbar()  # Add a colorbar for reference\n",
    "plt.xticks(range(len(XYZ_amount_cards_columns_filtered)), XYZ_amount_cards_columns_filtered, rotation=90)\n",
    "plt.yticks(range(len(XYZ_amount_cards_columns_filtered)), XYZ_amount_cards_columns_filtered)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation between 'Total amount Withdrawn' and selected columns\n",
    "correlation_values = atm_data_day[XYZ_amount_cards_columns_filtered].corrwith(atm_data_day['Total amount Withdrawn']).sort_values()\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "correlation_values.plot(kind='bar', color='blue')\n",
    "plt.title('Correlation between Total Amount Withdrawn and Other Columns')\n",
    "plt.ylabel('Correlation')\n",
    "\n",
    "# Increase spacing between labels\n",
    "plt.subplots_adjust(bottom=0.4)\n",
    "\n",
    "plt.xticks(rotation=80)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "    The generated correlation plots and bar plots, which focus on features related \n",
    "    to XYZ Card transactions, echo and reinforce the key insights observed in the \n",
    "    previous visualizations. These plots confirm that the correlations between the \n",
    "    'Total amount Withdrawn' and various XYZ Card-specific features, such as counts, \n",
    "    moving averages, and exponential moving averages, are consistent with the trends \n",
    "    identified earlier.\n",
    "\n",
    "    The high positive correlation between the 'Total amount Withdrawn' and these XYZ\n",
    "    Card-related features supports the notion that the transaction behaviors associated \n",
    "    with XYZ Card usage have a significant impact on the overall cash flow of ATMs. The \n",
    "    strength of these correlations underscores the potential predictive power that these \n",
    "    features hold for forecasting future cash flow dynamics.\n",
    "\n",
    "    Collectively, these analyses emphasize the robustness and reliability of the identified \n",
    "    insights. The consistent patterns observed across different categories of features \n",
    "    (including overall transaction counts, counts by card type, moving averages, and exponential \n",
    "    moving averages) provide a comprehensive view of the factors influencing ATM cash flow. \n",
    "    By leveraging these insights, data-driven predictions can be made more accurate and actionable, \n",
    "    facilitating better resource allocation and decision-making within the context of ATM management \n",
    "    and operations.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns that include 'XYZ'in their names\n",
    "Other_amount_cards_columns_filtered = [col for col in selected_columns if 'Other' in col]\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = atm_data_day[Other_amount_cards_columns_filtered].corr()\n",
    "\n",
    "# Create a heatmap-style correlation plot using matplotlib\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='none')\n",
    "plt.title('Correlation Plot')\n",
    "plt.colorbar()  # Add a colorbar for reference\n",
    "plt.xticks(range(len(Other_amount_cards_columns_filtered)), Other_amount_cards_columns_filtered, rotation=90)\n",
    "plt.yticks(range(len(Other_amount_cards_columns_filtered)), Other_amount_cards_columns_filtered)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation between 'Total amount Withdrawn' and selected columns\n",
    "correlation_values = atm_data_day[Other_amount_cards_columns_filtered].corrwith(atm_data_day['Total amount Withdrawn']).sort_values()\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "correlation_values.plot(kind='bar', color='blue')\n",
    "plt.title('Correlation between Total Amount Withdrawn and Other Columns')\n",
    "plt.ylabel('Correlation')\n",
    "\n",
    "# Increase spacing between labels\n",
    "plt.subplots_adjust(bottom=0.4)\n",
    "\n",
    "plt.xticks(rotation=80)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "    The correlation plots and bar plots generated for features associated with \n",
    "    \"Other Card\" transactions exhibit a similar pattern of insights as observed \n",
    "    in the previous analyses. These visualizations underscore the consistent \n",
    "    relationships between different aspects of \"Other Card\" transaction behavior \n",
    "    and the overall cash flow dynamics of ATMs.\n",
    "\n",
    "    The strong positive correlations observed between the 'Total amount Withdrawn' \n",
    "    and these \"Other Card\"-related features reinforce the notion that transaction \n",
    "    trends linked to \"Other Card\" usage play a pivotal role in shaping the overall \n",
    "    cash flow. This alignment between multiple types of features and their correlations \n",
    "    with the target variable emphasizes their predictive potential for forecasting \n",
    "    future cash flow patterns accurately.\n",
    "\n",
    "    The convergence of insights across various categories of features, including \n",
    "    transaction counts, moving averages, and exponential moving averages for \"Other \n",
    "    Card\" transactions, substantiates the robustness of the conclusions drawn. By \n",
    "    considering these insights collectively, data-driven predictions can be enriched \n",
    "    with a comprehensive understanding of the drivers behind ATM cash flow dynamics. \n",
    "    As a result, decision-makers can make informed choices that optimize the performance \n",
    "    and management of ATM networks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    The next step is to build a predictive model that can accurately forecast the cash flow for each ATM.\n",
    "\"\"\"\n",
    "atm_data_day.sort_index(inplace=True)\n",
    "atm_data_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    The features for the input data 'x' have been meticulously selected based \n",
    "    on a comprehensive analysis that involves evaluating various graphs and \n",
    "    conducting exploratory data analysis (EDA). Additionally, different data \n",
    "    selection techniques such as filter, wrapper, and intrinsic methods have \n",
    "    been employed to ensure the relevance and significance of the chosen features. \n",
    "    This approach aims to enhance the predictive power of the model by incorporating \n",
    "    attributes that exhibit meaningful correlations with the target variable \n",
    "    'Total amount Withdrawn'.\n",
    "\"\"\"\n",
    "\n",
    "# Selecting specific columns as features for the input data 'x'\n",
    "\n",
    "x = atm_data_day[['Weekday_Friday', 'Weekday_Monday', 'Weekday_Saturday',\n",
    "                  'Weekday_Sunday', 'Weekday_Thursday', 'Weekday_Tuesday',\n",
    "                  'Weekday_Wednesday', 'Working Day_H', 'Working Day_W', 'Total amount Withdrawn_lag_1',\n",
    "                  'Amount withdrawn XYZ Card_lag_1', 'Amount withdrawn Other Card_lag_1',\n",
    "                  'No Of Withdrawals_lag_1', 'No Of XYZ Card Withdrawals_lag_1',\n",
    "                  'No Of Other Card Withdrawals_lag_1', 'No Of Withdrawals_SMA_3',\n",
    "                  'No Of Withdrawals_SMA_7', 'No Of Withdrawals_SMA_10',\n",
    "                  'No Of XYZ Card Withdrawals_SMA_3', 'No Of XYZ Card Withdrawals_SMA_7',\n",
    "                  'No Of XYZ Card Withdrawals_SMA_10',\n",
    "                  'No Of Other Card Withdrawals_SMA_3',\n",
    "                  'No Of Other Card Withdrawals_SMA_7',\n",
    "                  'No Of Other Card Withdrawals_SMA_10', 'Total amount Withdrawn_SMA_3',\n",
    "                  'Total amount Withdrawn_SMA_7', 'Total amount Withdrawn_SMA_10',\n",
    "                  'Amount withdrawn XYZ Card_SMA_3', 'Amount withdrawn XYZ Card_SMA_7',\n",
    "                  'Amount withdrawn XYZ Card_SMA_10', 'Amount withdrawn Other Card_SMA_3',\n",
    "                  'Amount withdrawn Other Card_SMA_7',\n",
    "                  'Amount withdrawn Other Card_SMA_10', 'No Of Withdrawals_EMA_10',\n",
    "                  'No Of Withdrawals_EMA_30', 'No Of Withdrawals_EMA_90',\n",
    "                  'No Of XYZ Card Withdrawals_EMA_10',\n",
    "                  'No Of XYZ Card Withdrawals_EMA_30',\n",
    "                  'No Of XYZ Card Withdrawals_EMA_90',\n",
    "                  'No Of Other Card Withdrawals_EMA_10',\n",
    "                  'No Of Other Card Withdrawals_EMA_30',\n",
    "                  'No Of Other Card Withdrawals_EMA_90', 'Total amount Withdrawn_EMA_10',\n",
    "                  'Total amount Withdrawn_EMA_30', 'Total amount Withdrawn_EMA_90',\n",
    "                  'Amount withdrawn XYZ Card_EMA_10', 'Amount withdrawn XYZ Card_EMA_30',\n",
    "                  'Amount withdrawn XYZ Card_EMA_90',\n",
    "                  'Amount withdrawn Other Card_EMA_10',\n",
    "                  'Amount withdrawn Other Card_EMA_30',\n",
    "                  'Amount withdrawn Other Card_EMA_90']]\n",
    "\n",
    "# Selecting the 'Total amount Withdrawn' column as the target variable 'y'\n",
    "y = atm_data_day['Total amount Withdrawn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the length of DataFrame x\n",
    "print(\"Length of DataFrame x:\", len(x))\n",
    "\n",
    "# Print the length of DataFrame y\n",
    "print(\"Length of DataFrame y:\", len(y))\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "# Print the number of missing values in DataFrame x\n",
    "print(\"NULL values in DataFrame x:\")\n",
    "print(x.isna().sum())\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "# Print the number of missing values in DataFrame y\n",
    "print(\"NULL values in DataFrame y:\")\n",
    "print(y.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training, validation, and test sets based on date ranges\n",
    "\n",
    "# Select data up to '2016-01-02' for training\n",
    "x_train = x[:'2016-01-02']\n",
    "y_train = y[:'2016-01-02']\n",
    "\n",
    "# Select data from '2016-01-02' to '2017-01-02' for validation\n",
    "x_val = x['2016-01-02': '2017-01-02']\n",
    "y_val = y['2016-01-02': '2017-01-02']\n",
    "\n",
    "# Select data from '2017-01-02' onwards for testing\n",
    "x_test = x['2017-01-02':]\n",
    "y_test = y['2017-01-02':]\n",
    "\n",
    "# Select data up to '2017-01-02' for K-Fold model\n",
    "x_kfold = x[:'2017-01-02']\n",
    "y_kfold = y[:'2017-01-02']\n",
    "\n",
    "# Select data to '2017-01-02' for time series model\n",
    "x_time = x[:'2017-01-02']\n",
    "y_time = y[:'2017-01-02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Lazy Regressor to quickly evaluate multiple models\n",
    "reg = LazyRegressor()\n",
    "models_summary = reg.fit(x_train, x_val, y_train, y_val)\n",
    "\n",
    "# Print the summary report of model performance\n",
    "print(models_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of base models for stacking\n",
    "base_models = [\n",
    "    ('Lasso', Lasso()),\n",
    "    ('BayesianRidge', BayesianRidge()),\n",
    "    ('RidgeCV', RidgeCV()),\n",
    "    ('RANSACRegressor', RANSACRegressor()),\n",
    "    ('ridge', Ridge())\n",
    "]\n",
    "\n",
    "# Define a meta-model for stacking\n",
    "meta_model = LinearRegression()\n",
    "\n",
    "# Initialize lists to store base model predictions\n",
    "base_predictions_train = []\n",
    "base_predictions_val = []\n",
    "\n",
    "# Loop through each base model\n",
    "for name, model in base_models:\n",
    "    model.fit(x_train, y_train)\n",
    "    train_preds = model.predict(x_train)\n",
    "    val_preds = model.predict(x_val)\n",
    "    base_predictions_train.append(train_preds)\n",
    "    base_predictions_val.append(val_preds)\n",
    "\n",
    "# Stack the base model predictions horizontally\n",
    "stacked_train_preds = np.column_stack(base_predictions_train)\n",
    "stacked_val_preds = np.column_stack(base_predictions_val)\n",
    "\n",
    "# Fit the meta-model on stacked predictions\n",
    "meta_model.fit(stacked_train_preds, y_train)\n",
    "\n",
    "# Predictions using the meta-model on validation and training sets\n",
    "meta_val_preds = meta_model.predict(stacked_val_preds)\n",
    "meta_train_preds = meta_model.predict(stacked_train_preds)\n",
    "\n",
    "# Calculate and print scores and errors for validation and training sets\n",
    "score_valid = meta_model.score(stacked_val_preds, y_val)\n",
    "print('Validation Score:', score_valid)\n",
    "\n",
    "score_train = meta_model.score(stacked_train_preds, y_train)\n",
    "print('Training Score:', score_train)\n",
    "\n",
    "mse_valid = mean_squared_error(y_val, meta_val_preds)\n",
    "print('Validation Mean Squared Error:', mse_valid)\n",
    "\n",
    "mae_valid = mean_absolute_error(y_val, meta_val_preds)\n",
    "print('Validation Mean Absolute Error:', mae_valid)\n",
    "\n",
    "mse_train = mean_squared_error(y_train, meta_train_preds)\n",
    "print('Training Mean Squared Error:', mse_train)\n",
    "\n",
    "mae_train = mean_absolute_error(y_train, meta_train_preds)\n",
    "print('Training Mean Absolute Error:', mae_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store base model predictions for the test set\n",
    "base_predictions_test = []\n",
    "\n",
    "# Loop through each base model\n",
    "for name, model in base_models:\n",
    "    # Train each base model on the full training data (x_train, y_train)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    test_preds = model.predict(x_test)\n",
    "    base_predictions_test.append(test_preds)\n",
    "\n",
    "# Stack the base model predictions horizontally for the test set\n",
    "stacked_test_preds = np.column_stack(base_predictions_test)\n",
    "\n",
    "# Predict using the meta-model on stacked predictions for the test set\n",
    "meta_test_preds = meta_model.predict(stacked_test_preds)\n",
    "\n",
    "# Calculate and print scores and errors for the test set\n",
    "score_test = meta_model.score(stacked_test_preds, y_test)\n",
    "print('Test Score:', score_test)\n",
    "\n",
    "mse_test = mean_squared_error(y_test, meta_test_preds)\n",
    "print('Test Mean Squared Error:', mse_test)\n",
    "\n",
    "mae_test = mean_absolute_error(y_test, meta_test_preds)\n",
    "print('Test Mean Absolute Error:', mae_test)\n",
    "\n",
    "# Create a scatter plot for actual vs. predicted values on the test set\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, meta_test_preds, alpha=0.5)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values (Test)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Sort the data by index for visualization\n",
    "sorted_y_test = y_test.sort_index()\n",
    "sorted_meta_test_preds = pd.Series(meta_test_preds, index=y_test.index)\n",
    "sorted_meta_test_preds = sorted_meta_test_preds.sort_index()\n",
    "\n",
    "# Create a line plot for sorted actual values and sorted predicted values over time\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(sorted_y_test.index, sorted_y_test.values, label='Actual Values', linewidth=2)\n",
    "plt.plot(sorted_y_test.index, sorted_meta_test_preds, label='Predicted Values', linewidth=2)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Actual vs. Predicted Values Over Time (Test)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "    The close alignment between the validation, training, and test scores indicates \n",
    "    consistent performance across different datasets. This consistency is a positive \n",
    "    indicator that the model is neither significantly overfitting nor underfitting.\n",
    "\n",
    "    In conclusion, the stacked ensemble model demonstrates good generalization ability, \n",
    "    producing competitive results on both validation and test datasets. This suggests \n",
    "    that the ensemble's aggregated predictions from various base models are able to \n",
    "    effectively capture the underlying patterns in the data, leading to accurate and \n",
    "    reliable predictions on new data points.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear regression model\n",
    "model_reg = LinearRegression()\n",
    "model_reg.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_valid = model_reg.predict(x_val)\n",
    "\n",
    "# Calculate Mean Squared Error for validation set\n",
    "mse_valid = mean_squared_error(y_val, y_pred_valid)\n",
    "print('Validation Mean Squared Error:', mse_valid)\n",
    "\n",
    "# Calculate Mean Absolute Error for validation set\n",
    "mae_valid = mean_absolute_error(y_val, y_pred_valid)\n",
    "print('Validation Mean Absolute Error:', mae_valid)\n",
    "\n",
    "print('Train Score:', model_reg.score(x_train, y_train))\n",
    "print('Validation Score:', model_reg.score(x_val, y_val))\n",
    "\n",
    "# Create a scatter plot for actual vs. predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_val, y_pred_valid, alpha=0.5)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Sort the data by index\n",
    "sorted_y_val = y_val.sort_index()\n",
    "y_pred_valid = pd.Series(y_pred_valid, index=y_val.index)\n",
    "sorted_y_pred_valid = y_pred_valid.sort_index()\n",
    "\n",
    "# Create a line plot for sorted actual values and sorted predicted values over time\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(sorted_y_val.index, sorted_y_val.values, label='Actual Values', linewidth=2)\n",
    "plt.plot(sorted_y_val.index, sorted_y_pred_valid, label='Predicted Values', linewidth=2)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Actual vs. Predicted Values Over Time')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    The relatively close alignment between the average training and \n",
    "    validation scores is a positive sign, indicating that the model \n",
    "    is likely capturing meaningful patterns and not simply memorizing \n",
    "    the training data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model using the test set\n",
    "# Make predictions on the test set\n",
    "y_pred_test = model_reg.predict(x_test)\n",
    "\n",
    "# Calculate Mean Squared Error for test set\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "print('Test Mean Squared Error:', mse_test)\n",
    "\n",
    "# Calculate Mean Absolute Error for test set\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "print('Test Mean Absolute Error:', mae_test)\n",
    "\n",
    "# Print average score for the test set\n",
    "print('Test Score:', model_reg.score(x_test, y_test))\n",
    "\n",
    "# Create a scatter plot to visualize predicted vs. actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_test, alpha=0.5)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Sort the data by index for better visualization\n",
    "sorted_y_test = y_test.sort_index()\n",
    "y_pred_test = pd.Series(y_pred_test, index=y_test.index)\n",
    "sorted_y_pred_test = y_pred_test.sort_index()\n",
    "\n",
    "# Create a line plot to compare sorted actual values and sorted predicted values over time\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(sorted_y_test.index, sorted_y_test.values, label='Actual Values', linewidth=2)\n",
    "plt.plot(sorted_y_test.index, sorted_y_pred_test, label='Predicted Values', linewidth=2)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Actual vs. Predicted Values Over Time')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train an XGBoost regressor\n",
    "xgb_regressor = xgb.XGBRegressor(random_state=42)\n",
    "xgb_regressor.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_valid = xgb_regressor.predict(x_val)\n",
    "\n",
    "# Calculate Mean Squared Error for validation set\n",
    "mse_valid = mean_squared_error(y_val, y_pred_valid)\n",
    "print('Validation Mean Squared Error:', mse_valid)\n",
    "\n",
    "# Calculate Mean Absolute Error for validation set\n",
    "mae_valid = mean_absolute_error(y_val, y_pred_valid)\n",
    "print('Validation Mean Absolute Error:', mae_valid)\n",
    "\n",
    "# Calculate R-squared (R2) scores for training and validation sets\n",
    "r2_train = xgb_regressor.score(x_train, y_train)\n",
    "r2_valid = xgb_regressor.score(x_val, y_val)\n",
    "print('Average Train Score:', r2_train)\n",
    "print('Average Validation Score:', r2_valid)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Given the difference between the training and validation scores, \n",
    "    there exists some level of overfitting, where the model performs \n",
    "    exceptionally well on the training data but relatively less well on \n",
    "    unseen validation data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize k-fold cross-validation with 5 splits, shuffling, and a fixed random state\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_idx, val_idx in kfold.split(x_kfold):\n",
    "    # Split the data into training and validation sets based on fold indices\n",
    "    x_train, x_val = x_kfold.iloc[train_idx], x_kfold.iloc[val_idx]\n",
    "    y_train, y_val = y_kfold.iloc[train_idx], y_kfold.iloc[val_idx]\n",
    "    \n",
    "    # Train a linear regression model on the training data\n",
    "    KF_linear_model = LinearRegression()\n",
    "    KF_linear_model.fit(x_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation set\n",
    "    y_pred_val = KF_linear_model.predict(x_val)\n",
    "    \n",
    "    # Calculate evaluation metrics for this fold\n",
    "    mse = mean_squared_error(y_val, y_pred_val)\n",
    "    mae = mean_absolute_error(y_val, y_pred_val)\n",
    "    train_score = KF_linear_model.score(x_train, y_train)\n",
    "    val_score = KF_linear_model.score(x_val, y_val)\n",
    "    \n",
    "    # Append metrics to respective lists\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    train_scores.append(train_score)\n",
    "    val_scores.append(val_score)\n",
    "\n",
    "# Calculate and print the average metrics over all folds\n",
    "avg_mse = sum(mse_scores) / len(mse_scores)\n",
    "avg_mae = sum(mae_scores) / len(mae_scores)\n",
    "avg_train_score = sum(train_scores) / len(train_scores)\n",
    "avg_val_score = sum(val_scores) / len(val_scores)\n",
    "\n",
    "print('Average MSE:', avg_mse)\n",
    "print('Average MAE:', avg_mae)\n",
    "print('Average Train Score:', avg_train_score)\n",
    "print('Average Validation Score:', avg_val_score)\n",
    "\n",
    "\"\"\"\n",
    "    The closely aligned average training and validation scores suggest that the model \n",
    "    is effectively capturing patterns in the data and demonstrating a strong ability \n",
    "    to generalize to new data. These results are positive indicators of the model's \n",
    "    potential reliability in real-world scenarios.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test dataset\n",
    "y_pred_test = KF_linear_model.predict(x_test)\n",
    "\n",
    "# Calculate Mean Squared Error for the test set\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "print('Test Mean Squared Error:', mse_test)\n",
    "\n",
    "# Calculate Mean Absolute Error for the test set\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "print('Test Mean Absolute Error:', mae_test)\n",
    "\n",
    "# Calculate R-squared (R2) score for the test set\n",
    "r2_test = KF_linear_model.score(x_test, y_test)\n",
    "print('Average Test Score:', r2_test)\n",
    "\n",
    "# Create a scatter plot for actual vs. predicted values on the test set\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_test, alpha=0.5)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values (Test)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Sort the data by index for visualization\n",
    "sorted_y_test = y_test.sort_index()\n",
    "sorted_y_pred_test = pd.Series(y_pred_test, index=y_test.index)\n",
    "sorted_y_pred_test = sorted_y_pred_test.sort_index()\n",
    "\n",
    "# Create a line plot for sorted actual values and sorted predicted values over time\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(sorted_y_test.index, sorted_y_test.values, label='Actual Values', linewidth=2)\n",
    "plt.plot(sorted_y_test.index, sorted_y_pred_test, label='Predicted Values', linewidth=2)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Actual vs. Predicted Values Over Time (Test)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "    Given that the test R2 score is relatively close to the average train \n",
    "    and validation scores, it appears that the model's performance on the \n",
    "    test set is in line with its performance on the training and validation \n",
    "    sets. This consistency is a positive sign and suggests that the model \n",
    "    has not suffered from significant overfitting or underfitting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize k-fold cross-validation with 5 splits, shuffling, and a fixed random state\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics for each fold\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_idx, val_idx in kfold.split(x_kfold):\n",
    "    # Split the data into training and validation sets based on fold indices\n",
    "    x_train, x_val = x_kfold.iloc[train_idx], x_kfold.iloc[val_idx]\n",
    "    y_train, y_val = y_kfold.iloc[train_idx], y_kfold.iloc[val_idx]\n",
    "    \n",
    "    # Train an XGBoost regressor on the training data\n",
    "    KF_xgb_regressor = xgb.XGBRegressor(random_state=42)\n",
    "    KF_xgb_regressor.fit(x_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation set\n",
    "    y_pred_val = KF_xgb_regressor.predict(x_val)\n",
    "    \n",
    "    # Calculate evaluation metrics for this fold\n",
    "    mse = mean_squared_error(y_val, y_pred_val)\n",
    "    mae = mean_absolute_error(y_val, y_pred_val)\n",
    "    train_score = KF_xgb_regressor.score(x_train, y_train)\n",
    "    val_score = KF_xgb_regressor.score(x_val, y_val)\n",
    "    \n",
    "    # Append metrics to respective lists\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    train_scores.append(train_score)\n",
    "    val_scores.append(val_score)\n",
    "\n",
    "# Calculate and print the average metrics over all folds\n",
    "avg_mse = sum(mse_scores) / len(mse_scores)\n",
    "avg_mae = sum(mae_scores) / len(mae_scores)\n",
    "avg_train_score = sum(train_scores) / len(train_scores)\n",
    "avg_val_score = sum(val_scores) / len(val_scores)\n",
    "\n",
    "print('Average MSE:', avg_mse)\n",
    "print('Average MAE:', avg_mae)\n",
    "print('Average Train Score:', avg_train_score)\n",
    "print('Average Validation Score:', avg_val_score)\n",
    "\n",
    "\"\"\"\n",
    "    The noticeable difference between the average training and \n",
    "    validation scores suggests the possibility of overfitting. \n",
    "    The model seems to have been trained to fit the training \n",
    "    data very closely, which may lead to reduced performance \n",
    "    on new data. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize time series split\n",
    "tscv = TimeSeriesSplit(n_splits=2)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "\n",
    "# Perform time series cross-validation\n",
    "for train_idx, val_idx in tscv.split(x_time):\n",
    "    # Split the data into training and validation sets based on time series split indices\n",
    "    x_train, x_val = x_time.iloc[train_idx], x_time.iloc[val_idx]\n",
    "    y_train, y_val = y_time.iloc[train_idx], y_time.iloc[val_idx]\n",
    "    \n",
    "    # Train a linear regression model on the training data\n",
    "    time_series_reg_model = LinearRegression()\n",
    "    time_series_reg_model.fit(x_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation set\n",
    "    y_pred_val = time_series_reg_model.predict(x_val)\n",
    "    \n",
    "    # Calculate metrics for evaluation\n",
    "    mse = mean_squared_error(y_val, y_pred_val)\n",
    "    mae = mean_absolute_error(y_val, y_pred_val)\n",
    "    train_score = time_series_reg_model.score(x_train, y_train)\n",
    "    val_score = time_series_reg_model.score(x_val, y_val)\n",
    "    \n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    train_scores.append(train_score)\n",
    "    val_scores.append(val_score)\n",
    "\n",
    "# Calculate and print the average metrics over all folds\n",
    "avg_mse = sum(mse_scores) / len(mse_scores)\n",
    "avg_mae = sum(mae_scores) / len(mae_scores)\n",
    "avg_train_score = sum(train_scores) / len(train_scores)\n",
    "avg_val_score = sum(val_scores) / len(val_scores)\n",
    "\n",
    "print('Average MSE:', avg_mse)\n",
    "print('Average MAE:', avg_mae)\n",
    "print('Average Train Score:', avg_train_score)\n",
    "print('Average Validation Score:', avg_val_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the time series model on the entire training data (x_train, y_train)\n",
    "time_series_reg_model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_test = time_series_reg_model.predict(x_test)\n",
    "\n",
    "# Calculate Mean Squared Error for the test set\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "print('Test Mean Squared Error:', mse_test)\n",
    "\n",
    "# Calculate Mean Absolute Error for the test set\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "print('Test Mean Absolute Error:', mae_test)\n",
    "\n",
    "# Calculate R-squared (R2) score for the test set\n",
    "r2_test = time_series_reg_model.score(x_test, y_test)\n",
    "print('Average Test Score:', r2_test)\n",
    "\n",
    "# Create a scatter plot for actual vs. predicted values on the test set\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_test, alpha=0.5)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values (Test)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Sort the data by index for visualization\n",
    "sorted_y_test = y_test.sort_index()\n",
    "sorted_y_pred_test = pd.Series(y_pred_test, index=y_test.index)\n",
    "sorted_y_pred_test = sorted_y_pred_test.sort_index()\n",
    "\n",
    "# Create a line plot for sorted actual values and sorted predicted values over time\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(sorted_y_test.index, sorted_y_test.values, label='Actual Values', linewidth=2)\n",
    "plt.plot(sorted_y_test.index, sorted_y_pred_test, label='Predicted Values', linewidth=2)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Actual vs. Predicted Values Over Time (Test)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "    Given that the test R2 score is relatively close to the average train \n",
    "    and validation scores, it appears that the model's performance on the \n",
    "    test set is in line with its performance on the training and validation \n",
    "    sets. This consistency is a positive sign and suggests that the model \n",
    "    has not suffered from significant overfitting or underfitting. But,\n",
    "    due to the difference between the average training and test scores\n",
    "    which is not very small, the model might slighlty overfitting.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize time series split with 2 splits\n",
    "time_series_splitter = TimeSeriesSplit(n_splits=2)\n",
    "\n",
    "# Initialize lists to store evaluation metrics\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "\n",
    "# Perform time series cross-validation\n",
    "for train_idx, val_idx in time_series_splitter.split(x_time):\n",
    "    # Split the data into training and validation sets based on fold indices\n",
    "    x_train, x_val = x_time.iloc[train_idx], x_time.iloc[val_idx]\n",
    "    y_train, y_val = y_time.iloc[train_idx], y_time.iloc[val_idx]\n",
    "    \n",
    "    # Initialize and train an XGBoost regressor\n",
    "    xgb_time_split_model = xgb.XGBRegressor(random_state=42)\n",
    "    xgb_time_split_model.fit(x_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation set\n",
    "    y_pred_val = xgb_time_split_model.predict(x_val)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    mse = mean_squared_error(y_val, y_pred_val)\n",
    "    mae = mean_absolute_error(y_val, y_pred_val)\n",
    "    train_score = xgb_time_split_model.score(x_train, y_train)\n",
    "    val_score = xgb_time_split_model.score(x_val, y_val)\n",
    "    \n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    train_scores.append(train_score)\n",
    "    val_scores.append(val_score)\n",
    "\n",
    "# Calculate and print the average metrics over all folds\n",
    "avg_mse = sum(mse_scores) / len(mse_scores)\n",
    "avg_mae = sum(mae_scores) / len(mae_scores)\n",
    "avg_train_score = sum(train_scores) / len(train_scores)\n",
    "avg_val_score = sum(val_scores) / len(val_scores)\n",
    "\n",
    "print('Average MSE:', avg_mse)\n",
    "print('Average MAE:', avg_mae)\n",
    "print('Average Train Score:', avg_train_score)\n",
    "print('Average Validation Score:', avg_val_score)\n",
    "\n",
    "\"\"\"\n",
    "    These results indicate a significant risk of overfitting, as evidenced by the \n",
    "    substantial gap between the train and validation scores. The model seems to be \n",
    "    fitting the training data almost perfectly but struggles to generalize well to \n",
    "    unseen data, which is reflected in the lower validation score\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    In conclusion, after a thorough exploration of various predictive models and a \n",
    "    comprehensive evaluation of their performance, the chosen model for forecasting \n",
    "    ATM cash flow is Linear Regression with K-Fold cross-validation. This decision \n",
    "    is grounded in several key observations that validate the reliability of this \n",
    "    model for addressing the specific problem at hand.\n",
    "\n",
    "    Firstly, the Linear Regression model demonstrated the highest Test score (86%) \n",
    "    among the considered models. This achievement suggests that the model's predictive\n",
    "    capability extends beyond the training data and performs consistently well on \n",
    "    previously unseen data, which is a crucial aspect in ensuring the model's \n",
    "    generalization to real-world scenarios.\n",
    "\n",
    "    Furthermore, the model's performance exhibited the least disparity between Test\n",
    "    and Train Scores (91.5% - 86% = 5.5%). This careful balancing act is instrumental\n",
    "    in avoiding the overfitting of the model. By achieving a close alignment between \n",
    "    training and test performance, the Linear Regression model effectively captures \n",
    "    the underlying patterns in the data without succumbing to the pitfalls of \n",
    "    memorization or lack of adaptability to new information.\n",
    "\n",
    "    In addition, the Linear Regression model showcased the least Mean Absolute Error \n",
    "    (MAE) when compared to alternative models (69368.45). MAE is a critical metric as \n",
    "    it quantifies the average absolute difference between the predicted and actual \n",
    "    values. The lower MAE indicates that the model's predictions are consistently \n",
    "    closer to the actual values, implying a higher level of accuracy in its forecasts.\n",
    "\n",
    "    Taking into account the combined strengths of achieving the highest Test score, \n",
    "    mitigating overfitting concerns, and yielding the least MAE, the Linear Regression \n",
    "    model with K-fold cross-validation emerges as a reliable and robust solution for \n",
    "    the ATM cash flow forecasting problem. Its consistent performance across different \n",
    "    data subsets and its ability to capture meaningful patterns in the data position it \n",
    "    as a valuable tool for decision-making and planning in this context.\n",
    "\n",
    "    It is worth noting that while the Linear Regression model has demonstrated its \n",
    "    reliability within the current context, continued monitoring and periodic reevaluation \n",
    "    of its performance against changing data patterns is essential to ensure its continued \n",
    "    accuracy and relevance.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
